# é‹ç”¨å¤§å‹èªè¨€æ¨¡å‹å¾®èª¿æŠ€è¡“æå‡ ICD-10 è¨ºæ–·ç·¨ç¢¼é æ¸¬æº–ç¢ºç‡
> [!NOTE]
> **[ç¹é«”ä¸­æ–‡](#ç¹é«”ä¸­æ–‡)** | **[English Version](#english-version)**

<a name="ç¹é«”ä¸­æ–‡"></a>
## ğŸ“– å°ˆæ¡ˆç°¡ä»‹ (Introduction)

æœ¬å°ˆæ¡ˆæ—¨åœ¨æ¢ç´¢å¦‚ä½•é€é **å¤§å‹èªè¨€æ¨¡å‹ (LLM)** çš„å¾®èª¿æŠ€è¡“ï¼Œå¾ç—…æ‚£çš„è‡¨åºŠè³‡è¨Šï¼ˆå¦‚ç—…æ­·ã€ç”¨è—¥ç´€éŒ„ï¼‰ä¸­ç²¾æº–æ¨ä¼° **ICD-10 è¨ºæ–·ä»£ç¢¼**ã€‚

å‚³çµ±æ–¹æ³•ï¼ˆå¦‚ BERTï¼‰åœ¨è™•ç†è¤‡é›œé†«ç™‚è„ˆçµ¡æ™‚å¾€å¾€é¢è‡¨ç“¶é ¸ï¼ˆF1 Score ç´„ 0.25ï¼‰ï¼Œä¸”ç¼ºä¹å°ä¸­æ–‡é†«ç™‚å°ˆæ¥­çŸ¥è­˜çš„ç†è§£ã€‚æœ¬ç ”ç©¶åˆ©ç”¨ **MIMIC-IV** è³‡æ–™åº«ï¼Œçµåˆ **å¼·åŒ–å­¸ç¿’ (GRPO)** èˆ‡ **ç›£ç£å¼å¾®èª¿ (SFT)**ï¼Œè‡´åŠ›æ–¼æå‡é†«ç™‚ç·¨ç¢¼å·¥ä½œçš„è‡ªå‹•åŒ–æ•ˆç‡èˆ‡æº–ç¢ºæ€§ã€‚

### æ ¸å¿ƒç›®æ¨™
* **ç²¾æº–é æ¸¬**ï¼šå¾éçµæ§‹åŒ–è‡¨åºŠæ–‡æœ¬ä¸­æå–ç‰¹å¾µï¼Œæº–ç¢ºé æ¸¬ ICD-10 ä»£ç¢¼ã€‚
* **æ•ˆç‡æå‡**ï¼šæ¸›å°‘äººå·¥ç·¨ç¢¼æ™‚é–“ï¼Œè¼”åŠ©é†«ç™‚è¡Œæ”¿æµç¨‹ã€‚
* **æŠ€è¡“æ¢ç´¢**ï¼šé©—è­‰ GRPO (Group Relative Policy Optimization) èˆ‡ç›£ç£å¼å¾®èª¿ (Supervised Fine-Tuning) åœ¨å¤šæ¨™ç±¤åˆ†é¡ä»»å‹™ä¸­çš„å„ªå‹¢ã€‚

---

##  å°ˆæ¡ˆçµæ§‹ (Project Structure)

æœ¬å€‰åº«çš„æª”æ¡ˆçµæ§‹çµ„ç¹”å¦‚ä¸‹ï¼š

```text
.
â”œâ”€â”€ Qwen3 baseline/           # Qwen3 åŸºç¤æ¨¡å‹çš„ Baseline æ¸¬è©¦ä»£ç¢¼èˆ‡çµæœ
â”œâ”€â”€ RL GRPO/                  # æœ¬å°ˆæ¡ˆæ ¸å¿ƒï¼šä½¿ç”¨ GRPO æ¼”ç®—æ³•é€²è¡Œå¼·åŒ–å­¸ç¿’å¾®èª¿çš„å¯¦ä½œä»£ç¢¼
â”œâ”€â”€ SFT/                      # æœ¬å°ˆæ¡ˆæ ¸å¿ƒï¼šSupervised Fine-Tuning (ç›£ç£å¼å¾®èª¿) ç›¸é—œå¯¦é©—ä»£ç¢¼
â”œâ”€â”€ medgemma baseline/        # ä½¿ç”¨ Google MedGemma æ¨¡å‹ä½œç‚ºå°ç…§çµ„çš„æ¸¬è©¦ä»£ç¢¼
â”œâ”€â”€ gemini API baseline/      # ä½¿ç”¨ Gemini API é€²è¡Œæ¸¬è©¦çš„åŸºæº–ä»£ç¢¼
â”œâ”€â”€ data preprocessing/       # MIMIC-IV åŸå§‹è³‡æ–™æ¸…æ´—èˆ‡å‰è™•ç†è…³æœ¬
â”œâ”€â”€ train data construction/  # ä½¿ç”¨Gemini 2.5 Flashçš„æ¨¡ç¯„å›ç­”å»ºæ§‹è¨“ç·´é›†çš„è…³æœ¬ 
â”œâ”€â”€ result/                   # å­˜æ”¾æ‰€æœ‰æ¨¡å‹å›ç­”è·Ÿå¯¦é©—çµæœ
â”œâ”€â”€ baseline_summary.ipynb    # å½™æ•´å„ Baseline æ¨¡å‹è¡¨ç¾çš„åˆ†æç­†è¨˜æœ¬
â”œâ”€â”€ note_icd_data.jsonl       # è™•ç†å¾Œçš„è‡¨åºŠç­†è¨˜èˆ‡ ICD å°æ‡‰æ•¸æ“š
â”œâ”€â”€ requirements.txt          # å°ˆæ¡ˆä¾è³´å¥—ä»¶æ¸…å–®
â”œâ”€â”€ requirements_sft.txt      # sft å°ˆæ¡ˆä¾è³´å¥—ä»¶æ¸…å–®
â””â”€â”€ ...
```
##  æ–¹æ³•è«– (Methodology)

### 1. ç›£ç£å¼å¾®èª¿èˆ‡çŸ¥è­˜è’¸é¤¾ (SFT & Knowledge Distillation)
åœ¨é€²è¡Œå¼·åŒ–å­¸ç¿’ä¹‹å‰ï¼Œæˆ‘å€‘å…ˆé€éçŸ¥è­˜è’¸é¤¾å»ºç«‹é«˜å“è³ªçš„åŸºç¤èƒ½åŠ›ï¼š

* **æ•¸æ“šåˆæˆ**ï¼šåˆ©ç”¨ **Gemini 2.5 Flash** ä½œç‚ºæ•™å¸«æ¨¡å‹ï¼Œé‡å°é†«ç™‚æ¡ˆä¾‹ç”Ÿæˆå…·å‚™è©³ç´°æ¨ç†é‚è¼¯ï¼ˆChain of Thoughtï¼‰çš„æ¨¡ç¯„å›ç­”ã€‚
* **æ¨¡å‹è’¸é¤¾**ï¼šå°‡åˆæˆçš„æ¨ç†æ•¸æ“šè¼¸å…¥ **Qwen 14B** é€²è¡Œç›£ç£å¼å¾®èª¿ (SFT)ï¼Œä½¿å…¶å¸æ”¶æ•™å¸«æ¨¡å‹çš„é†«ç™‚æ¨ç†è·¯å¾‘èˆ‡è¨ºæ–·é‚è¼¯ã€‚
* **æŠ€è¡“å¯¦ç¾**ï¼šæ¡ç”¨ Unsloth æ¡†æ¶ã€‚é€éå…¶å„ªåŒ–çš„æ ¸å¿ƒèˆ‡ Triton ç®—å­ï¼Œæˆ‘å€‘åœ¨å¾®èª¿éç¨‹ä¸­æˆåŠŸæ¸›å°‘äº†ç´„ 70% çš„é¡¯å­˜ä½”ç”¨ï¼Œä¸¦æå‡äº† 2 å€ä»¥ä¸Šçš„è¨“ç·´é€Ÿåº¦ã€‚

### 2. å¼·åŒ–å­¸ç¿’å¾®èª¿ (GRPO)
åœ¨ SFT åŸºç¤ä¸Šï¼Œæ¡ç”¨ DeepSeek æå‡ºçš„ **GRPO (Group Relative Policy Optimization)** æ¼”ç®—æ³•ï¼š

* **å„ªå‹¢**ï¼šä¸åŒæ–¼å‚³çµ± PPOï¼ŒGRPO ä¸éœ€è¦é¡å¤–çš„ Critic Networkï¼ˆè©•è«–å®¶æ¨¡å‹ï¼‰ï¼Œç¯€çœäº†å¤§é‡çš„é‹ç®—è³‡æºï¼Œæ¥µå…¶é©åˆåœ¨æœ‰é™é¡¯å­˜ä¸‹è™•ç†å…·æœ‰å¤šé …å€™é¸ä»£ç¢¼çš„è¤‡é›œä»»å‹™ã€‚
* **æ©Ÿåˆ¶**ï¼šé‡å°è¨ºæ–·ä»£ç¢¼çš„ æ ¼å¼æ­£ç¢ºæ€§ã€ä»£ç¢¼åŒ¹é…ç²¾æº–åº¦ ä»¥åŠ æ¨ç†é‚è¼¯çš„ä¸€è‡´æ€§ è¨­å®šçå‹µå‡½æ•¸ï¼Œå¼·åˆ¶æ¨¡å‹åœ¨è¼¸å‡ºæ™‚ä¸åƒ…æä¾›æ­£ç¢ºç·¨ç¢¼ï¼Œé‚„å¿…é ˆæä¾›åˆç†çš„è‡¨åºŠè­‰æ“šæ”¯æŒã€‚

### 3. æç¤ºå·¥ç¨‹ (Prompt Engineering)
è¨­è¨ˆå°ˆç”¨çš„ System Prompt å°‡æ¨¡å‹å®šä½ç‚ºã€Œå°ˆæ¥­é†«ç™‚ç·¨ç¢¼ç¨½æ ¸å“¡ã€ï¼š

* **CoT å¼•å°**ï¼šå¼·åˆ¶æ¨¡å‹å¿…é ˆå…ˆåˆ†æç—…å²èˆ‡ç”¨è—¥ï¼Œæœ€å¾Œå†åˆ—å‡º ICD-10 Codeã€‚

* **è¼¸å‡ºç´„æŸ**ï¼šå®šç¾©æ¨™æº–åŒ–çš„ JSON æˆ–æ¨™ç±¤æ ¼å¼ï¼Œç¢ºä¿æ¨¡å‹é æ¸¬çµæœå¯è¢«ä¸‹æ¸¸è¡Œæ”¿ç³»çµ±ç›´æ¥è§£æã€‚

### 4. å¯¦é©—æ•¸æ“šé›†
* **ä¾†æº**ï¼šMIMIC-IV (MIT-LCP)
* **å…§å®¹**ï¼šæ¶µè“‹é‡ç—‡ç›£è­·ç—…æ‚£çš„ç—…å²ã€ä¸»è¨´ã€è©³ç´°ç”¨è—¥ç´€éŒ„åŠå°ˆæ¥­äººå“¡æ¨™è¨»çš„æ¨™æº– ICD-10 ä»£ç¢¼ã€‚

---

##  å¯¦é©—çµæœ (Results)

æˆ‘å€‘æ¯”è¼ƒäº†ä¸åŒæ¨¡å‹è¦æ¨¡èˆ‡æ–¹æ³•åœ¨ ICD-10 é æ¸¬ä»»å‹™ä¸Šçš„è¡¨ç¾ (F1 Score)ï¼š

| Model / Method | è¡¨ç¾ (Average F1 Score) | å‚™è¨» |
| :--- | :--- | :--- |
| **MedGemma 4B** | ~0.1064 | é‡å°é†«ç™‚å„ªåŒ–çš„åŸºç¤æ¨¡å‹ï¼Œä½†å¯¦éš›å¯¦é©—ä¹‹è¡¨ç¾æœ‰é™ |
| **Qwen3 4B** | ~0.1813 | é€šç”¨æ¨¡å‹ Baseline |
| **Qwen3 14B** | ~0.2539 | è¼ƒå¤§åƒæ•¸æ¨¡å‹è¡¨ç¾æ›´ä½³ |
| **Qwen3 14B + SFT** | ~0.2734 | SFT å¾Œæ¨¡å‹æ¨ç†æ ¼å¼ç›¸ç•¶ç©©å®š |
| **Qwen3 14B + SFT + RL** | ~0.2958 | RL å¾Œæ¨¡å‹é†«å­¸çŸ¥è­˜æ¨ç†è¡¨ç¾æ›´åŠ æå‡ |
| *Gemini 2.5 Flash* | *~0.3255* | *Teacher æ¨¡å‹ (çŸ¥è­˜ä¾†æº)* |

 **ğŸ’¡ çµæœåˆ†æ**ï¼š
 1. **å„ªåŒ–æˆæ•ˆ**ï¼šQwen3 14B åœ¨åŠ å…¥ RL (GRPO) å¾Œï¼ŒF1 åˆ†æ•¸è¼ƒåŸå§‹ç‰ˆæœ¬æå‡äº†ç´„ **16.5%**ã€‚
 2. **çŸ¥è­˜è’¸é¤¾**ï¼šSFT éšæ®µæˆåŠŸå°‡ Gemini çš„æ¨ç†èƒ½åŠ›è½‰ç§»è‡³ Qwen æ¨¡å‹ä¸­ï¼Œè§£æ±ºäº†æ ¼å¼ä¸ç©©å®šçš„å•é¡Œã€‚
 3. **æ€§èƒ½å·®è·**ï¼šç¶“éå„ªåŒ–çš„ Qwen3 14B è¡¨ç¾æœ‰æ•ˆæ¥è¿‘ Gemini 2.5 Flashï¼Œè­‰æ˜äº†ã€ŒSFT + GRPOã€æµç¨‹åœ¨ç‰¹å®šå‚ç›´é ˜åŸŸï¼ˆé†«ç™‚ç·¨ç¢¼ï¼‰çš„å¼·å¤§æ½›åŠ›ã€‚
---


##  å®‰è£èˆ‡ä½¿ç”¨ (Getting Started)

### ç’°å¢ƒéœ€æ±‚
```bash
pip install -r requirements.txt
pip install -r requirements_sft.txt
```
### ğŸ› ï¸ æ•¸æ“šæº–å‚™

è«‹ç¢ºä¿æ‚¨æ“æœ‰ **MIMIC-IV** çš„å­˜å–æ¬Šé™ï¼Œä¸¦å°‡åŸå§‹æª”æ¡ˆæ”¾å…¥ `data preprocessing` æŒ‡å®šçš„è·¯å¾‘ä¸­åŸ·è¡Œæ¸…æ´—è…³æœ¬ã€‚

---

##  å¼•ç”¨èˆ‡è‡´è¬ (Citation)

æœ¬å°ˆæ¡ˆä½¿ç”¨äº† **MIMIC-IV** è³‡æ–™åº«ï¼Œä¸¦çµåˆäº†é«˜æ•ˆçš„å¾®èª¿èˆ‡å¼·åŒ–å­¸ç¿’æŠ€è¡“ã€‚

* **Data Citation**: Johnson, A., Bulgarelli, L., Pollard, T., ... Mark, R. (2023). MIMIC-IV. PhysioNet.
* **Technical References**:
    * **Unsloth**: Used for efficient **SFT** (Supervised Fine-Tuning) and memory optimization during training.
    * **DeepSeek GRPO Algorithm**: Used for Group Relative Policy Optimization to enhance reasoning capabilities.
* **Last updated**: 2025/12


<a name="english-version"></a>
# Enhancing ICD-10 Diagnostic Coding Accuracy via LLM Fine-Tuning

## ğŸ“– Project Overview (Introduction)

This project explores how **large language model (LLM)** fine-tuning techniques can be used to accurately predict **ICD-10 diagnostic codes** from patientsâ€™ clinical information (such as clinical notes and medication records).

Traditional approaches (e.g., BERT-based models) often struggle with complex medical contexts, achieving limited performance (F1 score around 0.25), and lack sufficient understanding of Chinese medical terminology. Leveraging the **MIMIC-IV** database, this study integrates **Reinforcement Learning (GRPO)** and **Supervised Fine-Tuning (SFT)** to improve the automation efficiency and accuracy of medical coding.

### Core Objectives

* **Accurate Prediction**: Extract meaningful features from unstructured clinical text to precisely predict ICD-10 codes.
* **Efficiency Improvement**: Reduce manual coding time and support medical administrative workflows.
* **Technical Exploration**: Validate the effectiveness of GRPO (Group Relative Policy Optimization) and Supervised Fine-Tuning in multi-label classification tasks.

---

## ğŸ“‚ Project Structure

The repository is organized as follows:

```text
.
â”œâ”€â”€ Qwen3 baseline/            # Baseline experiments and results using the Qwen3 base model
â”œâ”€â”€ RL GRPO/                   # Core component: reinforcement learning fine-tuning with the GRPO algorithm
â”œâ”€â”€ SFT/                       # Core component: Supervised Fine-Tuning (SFT) experiments
â”œâ”€â”€ medgemma baseline/         # Baseline experiments using Google MedGemma as a comparison model
â”œâ”€â”€ gemini API baseline/       # Baseline experiments using the Gemini API
â”œâ”€â”€ data preprocessing/        # Scripts for cleaning and preprocessing raw MIMIC-IV data
â”œâ”€â”€ train data construction/   # Scripts for constructing training data using exemplar answers from Gemini 2.5 Flash
â”œâ”€â”€ result/                    # Model outputs and experimental results
â”œâ”€â”€ baseline_summary.ipynb     # Notebook summarizing and analyzing baseline model performance
â”œâ”€â”€ note_icd_data.jsonl        # Processed clinical notes paired with ICD-10 codes
â”œâ”€â”€ requirements.txt           # Project dependencies
â”œâ”€â”€ requirements_sft.txt       # Dependencies specific to SFT experiments
â””â”€â”€ ...
```

---

## ğŸ§  Methodology

### 1. Supervised Fine-Tuning & Knowledge Distillation (SFT)

Before reinforcement learning, we first establish strong baseline capabilities through knowledge distillation:

* **Data Synthesis**: Use **Gemini 2.5 Flash** as a teacher model to generate exemplar answers with detailed reasoning chains (Chain of Thought) for medical cases.
* **Model Distillation**: Feed the synthesized reasoning data into **Qwen 14B** for Supervised Fine-Tuning (SFT), enabling the model to absorb the teacherâ€™s medical reasoning pathways and diagnostic logic.
* **Technical Implementation**: The **Unsloth** framework is adopted. With its optimized core and Triton kernels, GPU memory usage is reduced by approximately 70%, while training speed is increased by more than 2Ã—.

### 2. Reinforcement Learning Fine-Tuning (GRPO)

Building on the SFT model, we further apply **GRPO (Group Relative Policy Optimization)** proposed by DeepSeek:

* **Advantages**: Unlike traditional PPO, GRPO does not require an additional critic network, significantly reducing computational overhead. This makes it particularly suitable for complex tasks with multiple candidate codes under limited GPU memory.
* **Mechanism**: Reward functions are designed to enforce **output format correctness**, **code matching accuracy**, and **consistency of clinical reasoning**, ensuring that the model not only outputs correct ICD-10 codes but also provides clinically plausible justifications.

### 3. Prompt Engineering

A dedicated system prompt is designed to position the model as a *professional medical coding auditor*:

* **Chain-of-Thought Guidance**: The model is required to first analyze patient history and medications, and only then output the ICD-10 codes.
* **Output Constraints**: Standardized JSON or tagged formats are enforced so that predictions can be directly parsed by downstream administrative systems.

### 4. Experimental Dataset

* **Source**: MIMIC-IV (MIT-LCP)
* **Content**: Includes ICU patient histories, chief complaints, detailed medication records, and professionally annotated ICD-10 codes.

---

## ğŸ“Š Experimental Results

We compare different model sizes and methods on the ICD-10 prediction task using the F1 score:

| Model / Method           | Performance (Average F1 Score) | Notes                                                             |
| :----------------------- | :----------------------------- | :---------------------------------------------------------------- |
| **MedGemma 4B**          | ~0.1064                        | Medical-optimized base model, but limited performance in practice |
| **Qwen3 4B**             | ~0.1813                        | General-purpose baseline model                                    |
| **Qwen3 14B**            | ~0.2539                        | Larger model achieves better performance                          |
| **Qwen3 14B + SFT**      | ~0.2734                        | More stable reasoning format after SFT                            |
| **Qwen3 14B + SFT + RL** | ~0.2958                        | Further improvement in medical reasoning after RL                 |
| *Gemini 2.5 Flash*       | *~0.3255*                      | *Teacher model (knowledge source)*                                |

### ğŸ’¡ Result Analysis

1. **Optimization Effectiveness**: After applying GRPO-based RL, Qwen3 14B achieves an approximately **16.5%** improvement in F1 score compared to the original model.
2. **Knowledge Distillation**: The SFT stage successfully transfers Geminiâ€™s reasoning ability to the Qwen model, resolving output format instability.
3. **Performance Gap**: The optimized Qwen3 14B model approaches the performance of Gemini 2.5 Flash, demonstrating the strong potential of the **SFT + GRPO** pipeline in domain-specific tasks such as medical coding.

---

## ğŸš€ Getting Started

### Environment Setup

```bash
pip install -r requirements.txt
pip install -r requirements_sft.txt
```

### ğŸ› ï¸ Data Preparation

Please ensure that you have authorized access to **MIMIC-IV**, and place the raw files into the designated paths under `data preprocessing` before running the cleaning scripts.

---

## ğŸ“š Citation & Acknowledgements

This project utilizes the **MIMIC-IV** database and integrates efficient fine-tuning and reinforcement learning techniques.

* **Data Citation**: Johnson, A., Bulgarelli, L., Pollard, T., ... Mark, R. (2023). *MIMIC-IV*. PhysioNet.

* **Technical References**:

  * **Unsloth**: Used for efficient **SFT (Supervised Fine-Tuning)** and GPU memory optimization during training.
  * **DeepSeek GRPO Algorithm**: Used for Group Relative Policy Optimization to enhance reasoning capabilities.

* **Last updated**: 2025/12
